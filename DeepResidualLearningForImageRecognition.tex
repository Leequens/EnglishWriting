\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{enumerate}
\usepackage{times}
\usepackage{epsfig}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{array}
\usepackage{cite}
\usepackage{calc}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\title{\textbf{Deep Residual Learning for Image Recognition}}
\author{Zhang, Liqiang\\\\June 30, 2018}
\cvprfinalcopy
\def\cvprPaperID{****}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\begin{document}
\maketitle
\par
\begin{abstract}
  Deeper neural networks are more difficult to train. So this paper's author present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. They explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. The author provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. The depth of representations is of central importance for many visual recognition tasks. 
\end{abstract}
\section{Introduction}
Deep convolutional neural networks \cite{Krizhevsky2012ImageNet} have led to a series of breakthroughs for image classification. Recent evidence reveals that network depth is of crucial importance, and the leading results on the challenging ImageNet dataset all exploit ``very deep'' models, with a depth of sixteen to thirty. Many other nontrivial
visual recognition tasks \cite{He2015Spatial} have also greatly benefited from very deep models.
\par
Driven by the significance of depth, a question arises: \emph{Is learning better networks as easy as stacking more layers?} An obstacle to answering this question was the notorious problem of vanishing/exploding gradients, which hamper convergence from the beginning. This problem, however, has been largely addressed by normalized initialization and intermediate normalization layers, which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation.
\par
When deeper networks are able to start converging, a \emph{degradation} problem has been exposed: with the network
depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as reported in \cite{He2015Convolutional} and thoroughly verified by the experiments. Fig.~\ref{1} shows a typical example.
\begin{figure}
\begin{center}
  % Requires \usepackage{graphicx}
  \includegraphics[height=3.1cm,width=8.5cm]{1.png}\\
  \caption{Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer ``plai'' networks. The deeper network has higher training error, and thus test error.}\label{1}
\end{center}
\end{figure}
\par
In this paper, the author address the degradation problem by introducing a \emph{deep residual learning} framework. Instead
of hoping each few stacked layers directly fit a desired underlying mapping, they explicitly let these layers fit a residual mapping. Formally, denoting the desired underlying mapping as $\mathcal{H}(x)$, the author let the stacked nonlinear layers fit another mapping of $\mathcal{F}(x):=\mathcal{H}(x)-x$. The original mapping is recast into $\mathcal{F}(x)+x$. The author hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.
\section{Identity Mapping by Shortcuts}
The author adopt residual learning to every few stacked layers. Formally, in this paper they consider a building block defined as Eq.~\ref{11}:
\begin{equation}\label{11}
\begin{split}
y=\mathcal{F}(x,\{W_i\})+x.
\end{split}
\end{equation}
Here $x$ and $y$ are the input and output vectors of the layers considered. The function $\mathcal{F}(x,\{W_i\})$ represents the residual mapping to be learned. For the example in Fig.~\ref{2} that has two layers, $\mathcal{F}=W_2\sigma(W_1x)$ in which $\sigma$ denotes ReLU \cite{Nair2010Rectified} and the biases are omitted for simplifying notations. The operation $\mathcal{F}+x$ is performed by a shortcut connection and element-wise addition.
\begin{figure}
\begin{center}
  % Requires \usepackage{graphicx}
  \includegraphics[height=3.1cm,width=5.5cm]{2.png}\\
  \caption{Residual learning: a building block.}\label{2}
\end{center}
\end{figure}
\par
The dimensions of $x$ and $\mathcal{F}$ must be equal in Eq.~\ref{11}. If this is not the case, it could  perform a linear projection Ws by the shortcut connections to match the dimensions as Eq.~\ref{12}:
\begin{equation}\label{12}
\begin{split}
y=\mathcal{F}(x,\{W_i\})+W_sx.
\end{split}
\end{equation}
\par
The form of the residual function $\mathcal{F}$ is flexible. Experiments in this paper involve a function $\mathcal{F}$ that has two or three layers, while more layers are possible. But if $\mathcal{F}$ has only a single layer, Eq.~\ref{11} is similar to a linear layer: $y=W_1x+x$.
\section{Experiments}
\begin{table}[htbp]
\small
\renewcommand\arraystretch{1.2}
\centering
\begin{tabular}{|c|c|c|}
\hline
model & top-1 err.& top-5 err. \\
\hline
VGG-16 & 28.07 & 9.33 \\
GoogLeNet \cite{Szegedy2014Going} & - & 9.15 \\
PReLU-net & 24.27 & 7.38 \\
\hline
\hline
plain-34 & 28.54 & 10.02 \\
ResNet-34 A & 25.03 & 7.76 \\
ResNet-34 B & 24.52 & 7.46 \\
ResNet-34 C & 24.19 & 7.40 \\
\hline
ResNet-50 & 22.85 & 6.71 \\
ResNet-101 & 21.75 & 6.05 \\
ResNet-152 & \textbf{21.43} & \textbf{5.71} \\
\hline
\end{tabular} 
\caption{Error rates (\%, \textbf{10-crop} testing) on ImageNet validation. VGG-16 is based on our test. ResNet-50/101/152 are of option B that only uses projections for increasing dimensions.} \label{tab1}
\end{table}
Table~\ref{tab1} shows that all three options are considerably better than the plain counterpart. B is slightly better than A. The author argue that this is because the zero-padded dimensions in A indeed have no residual learning. C is marginally better than B, and they attribute this to the extra parameters introduced by many (thirteen) projection shortcuts. But the small differences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem. 
{\small
\bibliographystyle{ieee}
\bibliography{1}
}
\end{document}