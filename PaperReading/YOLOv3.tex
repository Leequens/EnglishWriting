\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{enumerate}
\usepackage{times}
\usepackage{epsfig}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{array}
\usepackage{cite}
\usepackage{calc}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\title{\textbf{YOLOv3: An Incremental Improvement}}
\author{Zhang, Liqiang\\\\July 20, 2018}
\cvprfinalcopy
\def\cvprPaperID{****}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\begin{document}
\maketitle
\par
\begin{abstract}
It's my honour to take part in underwater robot picking contest with classmates. My task is to collocation environment for YOLOv3, so I read a paper this week about YOLOv3. YOLOv3 is still fast but more accurate. At $320\times 320$ YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. YOLOv3 is quite good for the old .5 IOU mAP detection metric. It achieves 57.9 AP$_{50}$ in 51 ms on a Titan X, compared to 57.5 AP$_{50}$ in 198 ms by RetinaNet, similar performance but 3.8 $\times$ faster. All the code is online at \url{https://pjreddie.com/yolo/}.
\end{abstract}
\section{Introduction}
Under the background of GANS, the author has made some improvements to the previous YOLO algorithm. 
\par
In this paper, first The author tell us what the deal is with YOLOv3. Then author tell us how they do. They also tell about some things they tried that didn’t work. Finally They will contemplate what this all means.
\section{Bounding Box Prediction}
Here’s the deal with YOLOv3: The author trained a new classifier network that’s better than the other ones. Fig.~\ref{1} can show the speed of YOLOv3. 
\begin{figure}
\begin{center}
  % Requires \usepackage{graphicx}
  \includegraphics[height=5.5cm]{1.png}\\
  \caption{YOLOv3 runs significantly faster than other detection methods
with comparable performance. Times from either an M40 or Titan
X, they are basically the same GPU.}\label{1}
\end{center}
\end{figure}
\subsection{Bounding Box Prediction}
Following YOLO9000 the our system predicts bounding boxes using dimension clusters as anchor boxes. The network predicts 4 coordinates for each bounding box, $t_x, t_y, t_w, t_h$. If the cell is offset from the top left corner of the image by ($c_x, c_y$) and the bounding box prior has width and height $p_w, p_h$, then the predictions correspond to the Eq.~\ref{11}:
\begin{equation}\label{11}
\begin{split}
b_x=\sigma(t_x)+c_x \\
b_y=\sigma(t_y)+c_y \\
b_w=p_we^{t_w} \\
b_h=p_he^{t_h} \\
\end{split}
\end{equation}
\par
YOLOv3 predicts an objectness score for each bounding box using logistic regression. This should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior. If the bounding box prior is not the best but does overlap a ground truth object by more than some threshold the author ignore the prediction, following \cite{Redmon2017YOLO9000}
\subsection{Class Prediction}
Each box predicts the classes the bounding box may contain using multilabel classification. The author do not use a softmax as they have found it is unnecessary for good performance, instead they simply use independent logistic classifiers. During training they use binary cross-entropy loss for the class
predictions.
\par 
In this dataset there are many overlapping labels. Using a softmax imposes the assumption that each box has exactly one class which is often not the case. A multilabel approach better models the data.
\begin{table*}
\small
\renewcommand\arraystretch{1.2}
\centering
\begin{tabular}{c|c|ccc|ccc}
 & backbone & AP & $AP_{50}$ & $AP_{75}$ & $AP_S$ & $AP_M$ & $AP_L$\\
\hline 
\emph{Two-stage methods} \\
Faster R-CNN+++\cite{He2015Deep}& ResNet-101-C4 & 34.9 & 55.7 & 37.4 & 15.6 & 38.7 & 50.9 \\
Faster R-CNN w FPH\cite{Lin2016Feature} & ResNet-101-FPH & 36.2 & 59.1 & 39.0 & 18.2 & 39.0 & 48.2 \\
Faster R-CNN by G-RMI & Inception-ResNet-v2 & 34.7 & 55.5 & 36.7 & 13.5 & 38.1 & \textbf{52.0} \\
\hline
\emph{One-stage methods} \\
YOLOv2 \cite{Redmon2017YOLO9000} & DarkNet-19 & 21.6 & 44.0 & 19.2 & 5.0 & 22.4 & 35.5 \\
SSD513 & ResNet-101-SSD & 31.2 & 50.4 & 33.3 & 10.2 & 34.5 & 49.8 \\
DSSD513 & ResNet-101-DSSD & 33.2 & 53.3 & 35.2 & 13.0 & 35.4 & 51.1\\
RetinaNet & ResNeXt-101-FPN & \textbf{40.8} & \textbf{61.1} & \textbf{44.1} & \textbf{24.1} & \textbf{44.2} & 51.2 \\
YOLOv3 608$\times$ 608 & Darknet-53 & 33.0 & 57.9 & 34.4 & 18.3 & 35.4 & 41.9 \\
\end{tabular} 
\caption{YOLOv3 is much better than SSD variants and comparable to state-of the-art models on the $AP_{50}$ metric.}
\label{tab1}
\end{table*}
\section{How to do}
The Table~\ref{tab1} shows the performance of YOLOv3. In terms of COCOs weird average mean AP metric it is on par with the SSD variants but is 3$\times$ faster. It is still quite a bit behind other models like RetinaNet in this metric though.
\par
In the past YOLO struggled with small objects. However, now the author see a reversal in that trend. With the new multi-scale predictions they see YOLOv3 has relatively high $AP_S$ performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.

{\small
\bibliographystyle{ieee}
\bibliography{YOLOv3}
}

\end{document}